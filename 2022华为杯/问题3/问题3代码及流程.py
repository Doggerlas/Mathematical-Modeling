# -*- coding: utf-8 -*-
"""问题3代码及流程.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EWskMI89-601ru7eGClwZiL6LWiAk4rT

# **问题3**
"""

! pip install --upgrade xlrd
! pip install pandas==1.2.0
! pip install xgboost
! pip install lightgbm 
! pip install matplotlib 
! pip install mplfonts 
! pip install pybaobabdt 
! apt install libgraphviz-dev
! pip install pygraphviz
! pip install pydotplus

import warnings 
import chardet
import pybaobabdt
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
import numpy as np 
from sklearn.metrics import r2_score,classification_report,f1_score,make_scorer
from sklearn.model_selection import train_test_split,GridSearchCV,KFold
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from mplfonts.bin.cli import init
init()
from mplfonts import use_font
use_font('Noto Serif CJK SC')#指定中文字体
from sklearn.tree import DecisionTreeRegressor

# 附件14是12 14 16 18 20年
# 附件15是16 17 18 19 20年
# 只合并相同的16 18 20年
attachment_14 = pd.read_excel('附件14不同放牧强度土壤碳氮监测数据.xlsx')
tmp1=attachment_14[attachment_14.year==2020]
tmp2=attachment_14[attachment_14.year==2018]
tmp3=attachment_14[attachment_14.year==2016]
attachment_14=pd.concat([tmp1,tmp2,tmp3],axis=0)
attachment_14

# 输出文件原格式
def  GetEncodingSheme(_filename):
    with open(_filename, 'rb') as file:
        buf = file.read()
    result = chardet.detect(buf)
    return result['encoding']

# 原格式转换为utf-8
def ChangeEncoding(_infilename, _outfilname, _encodingsheme='utf-8'):
  ifEncodeSheme = GetEncodingSheme(_infilename)
  with open(_infilename, 'r', encoding=ifEncodeSheme) as fr:
    tempContent = fr.read()
  with open(_outfilname, 'w', encoding=_encodingsheme) as fw:
    fw.write(tempContent)

filename = '附件15群落结构监测数据集.csv'
print('原格式为：', GetEncodingSheme(filename)) #输出原格式
ChangeEncoding('附件15群落结构监测数据集.csv', '附件15群落结构监测数据集_new.csv', 'utf-8') #格式转化为utf-8
print('文件已转换为utf-8编码')

f=open('附件15群落结构监测数据集_new.csv')
attachment_15=pd.read_csv(f,encoding='utf-8')
tmp1=attachment_15[attachment_15.年份==2020]
tmp2=attachment_15[attachment_15.年份==2018]
tmp3=attachment_15[attachment_15.年份==2016]
attachment_15=pd.concat([tmp1,tmp2,tmp3],axis=0)
attachment_15

# 附件14 15 生成合并索引bloar=小区-年份 如G6-2012
attachment_14['bloar']=attachment_14.apply(lambda x:x['放牧小区（plot）']+'-'+str(x['year']),axis=1)
attachment_15['bloar']=attachment_15.apply(lambda x:x['放牧小区Block']+'-'+str(x['年份']),axis=1)

# 以bloar列为索引 合并附件14 15
attachment_1415=pd.merge(attachment_14,attachment_15,on='bloar')
attachment_1415

# 不同列缺失值数量 生殖苗过多 认为是无关项
attachment_1415.isnull().sum()

# 人为保留相关项
# '放牧小区（plot）', '轮次'                        ——放牧方式 只考虑选择划区轮牧
# '放牧强度（intensity）'）                         ——放牧强度 附件14的放牧强度(NG LGI MGI HGI)和附件15的处理(无牧（0天） 轻牧（3天） 中牧（6天）重牧（12天）)完全对应 仅保留一个就行
#  'SOC土壤有机碳', 'SIC土壤无机碳','STC土壤全碳', '全氮N', '土壤C/N比' ——化学性质
attachment_1415=attachment_1415[['年份', '放牧小区（plot）', '轮次',
                  '放牧强度（intensity）',
                  'SOC土壤有机碳', 'SIC土壤无机碳','STC土壤全碳', '全氮N', '土壤C/N比']]

plt.hist(attachment_1415['放牧强度（intensity）'], bins=50, color='steelblue')
plt.xlabel('放牧强度（intensity）')
plt.ylabel('数量')

plt.hist(attachment_1415['轮次'], bins=50, color='steelblue')
plt.xlabel('轮次')
plt.ylabel('数量')

plt.hist(attachment_1415['放牧小区（plot）'], bins=50, color='steelblue')
plt.xlabel('放牧小区（plot）')
plt.ylabel('数量')

# 放牧方式量化
# 将小区号 轮次号进行one-hot编码 1代表attachment_1415本行原本是该小区号/轮次号
attachment_1415=pd.concat([attachment_1415,pd.get_dummies(attachment_1415['放牧小区（plot）'],prefix='小区号_')],axis=1)#12列
attachment_1415=pd.concat([attachment_1415,pd.get_dummies(attachment_1415['轮次'],prefix='轮次_')],axis=1)#5列
attachment_1415

# 放牧强度量化
# 对照（NG， 0羊/天/公顷 ）、轻度放牧强度（LGI， 2羊/天/公顷 ）、中度放牧强度（MGI，4羊/天/公顷 ）和重度放牧强度（HGI，8羊/天/公顷 ）
attachment_1415['放牧强度（intensity）']=attachment_1415['放牧强度（intensity）'].map(
{
    'NG':0,
    'LGI':2, 
    'MGI':4, 
    'HGI':8
})
attachment_1415

# 重新选择
attachment_1415=attachment_1415[['年份', '放牧强度（intensity）',
       '小区号__G11', '小区号__G12', '小区号__G13','小区号__G16', '小区号__G17', '小区号__G18', '小区号__G19', '小区号__G20','小区号__G21', '小区号__G6', '小区号__G8', '小区号__G9', 
       '轮次__牧前', '轮次__第一轮牧后', '轮次__第二轮牧后','轮次__第三轮牧后', '轮次__第四轮牧后',
       'SOC土壤有机碳', 'SIC土壤无机碳','STC土壤全碳', '全氮N', '土壤C/N比'
       ]]
attachment_1415

attachment_1415.isnull().sum()/attachment_1415.shape[0]#计算缺失比例

# 3sigma 去极值
def filter_extreme_3sigma(dataframe,n=3):
  for i in dataframe.columns:
    mean=dataframe[i].mean()
    std=dataframe[i].std()
    max_range=mean+n*std
    min_range=mean-n*std
    dataframe[i] = pd.DataFrame(np.clip(dataframe[i].values, min_range, max_range), columns=None)
  return dataframe
attachment_1415=filter_extreme_3sigma(attachment_1415)

# 需要预测的数据
data = pd.read_excel('需要预测的值.xlsx')
data = pd.concat([data,pd.get_dummies(data['放牧小区（plot）'],prefix='小区号_')],axis=1)#12列
data['放牧强度（intensity）']=data['放牧强度（intensity）'].map(
{
    'NG':0,
    'LGI':2, 
    'MGI':4, 
    'HGI':8
})
data = data[['放牧强度（intensity）',
       '小区号__G11', '小区号__G12', '小区号__G13','小区号__G16', '小区号__G17', '小区号__G18', '小区号__G19', '小区号__G20','小区号__G21', '小区号__G6', '小区号__G8', '小区号__G9', 
       'SOC土壤有机碳', 'SIC土壤无机碳','STC土壤全碳', '全氮N', '土壤C/N比'
       ]]
data

# 模型评估
def performance_metric(y_true, y_predict):
    """计算并返回预测值相比于预测值的分数"""
    score = r2_score(y_true,y_predict)
    return score

def fit_model(X, y):
    """ 基于输入数据 [X,y]，利于网格搜索找到最优的决策树模型"""
    cross_validator = KFold()
    regressor = DecisionTreeRegressor()
    params = {"max_depth":np.arange(1,11)}
    scoring_fnc = make_scorer(performance_metric)
    grid = GridSearchCV(regressor,params,scoring=scoring_fnc)
    # 基于输入数据 [X,y]，进行网格搜索
    grid = grid.fit(X, y)
    print(pd.DataFrame(grid.cv_results_))
    # 返回网格搜索后的最优模型
    return grid.best_estimator_

# 模型训练
X=attachment_1415[['放牧强度（intensity）',
  '小区号__G11', '小区号__G12', '小区号__G13','小区号__G16', '小区号__G17', '小区号__G18', '小区号__G19', '小区号__G20','小区号__G21', '小区号__G6', '小区号__G8', '小区号__G9']]
from IPython.display import Image  
from sklearn import tree
import pydotplus 

y_SOC=attachment_1415['SOC土壤有机碳']
y_SIC=attachment_1415['SIC土壤无机碳']
y_STC=attachment_1415['STC土壤全碳']
y_N =attachment_1415['全氮N']
y_CN=attachment_1415['土壤C/N比']

X_data=data[['放牧强度（intensity）',
  '小区号__G11', '小区号__G12', '小区号__G13','小区号__G16', '小区号__G17', '小区号__G18', '小区号__G19', '小区号__G20','小区号__G21', '小区号__G6', '小区号__G8', '小区号__G9']]

count=0
name=['y_SOC','y_SIC','y_STC','y_N','y_CN']
for y in [y_SOC,y_SIC,y_STC,y_N,y_CN] :
  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)
  
  print('当前预测指标为：',name[count])
  

  #训练拟合
  # 决策树 基于训练数据，获得最优模型
  optimal_model = fit_model(x_train,y_train)
  print('最优模型', optimal_model)
  dot_data = tree.export_graphviz(optimal_model, out_file=None)
  graph = pydotplus.graph_from_dot_data(dot_data)
  s=name[count]+'.png'
  graph.write_png(s)
  # 输出最优模型的 'max_depth' 参数
  print("Parameter 'max_depth' is {} for the optimal model.".format(optimal_model.get_params()['max_depth']))

  #验证
  y_pred=optimal_model.predict(x_test)
  #print('预测值：',y_pred)
  #print('真实值：',y_test)
  #print(classification_report(y_pred,y_test.))

  #模型评估
  score = performance_metric(y_test,y_pred)
  print('score:', score)
  #print('coef_', optimal_model.coef_)
  #print('intercept_', optimal_model.intercept_)

  # 推理
  y_data=optimal_model.predict(X_data)
  print('需要预测的值：',y_data)

  count+=1